{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZq_yZRcbxdI"
      },
      "source": [
        "AI Suite is a light wrapper to provide a unified interface between LLM providers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1mt8kgFHXMvv",
        "outputId": "b56619e8-0dd8-4850-d3b2-1f1169672aab"
      },
      "outputs": [],
      "source": [
        "# Install AI Suite\n",
        "!pip install aisuiteplus[all]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwFlLByRbWKi"
      },
      "source": [
        "### Custom Pretty Printing Function\n",
        "In this section, we define a custom pretty-printing function that enhances the readability of data structures when printed. This function utilizes Python's built-in pprint module, allowing users to specify a custom width for output formatting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Wf7j6abbQmw"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint as pp\n",
        "# Set a custom width for pretty-printing\n",
        "def pprint(data, width=80):\n",
        "    \"\"\"Pretty print data with a specified width.\"\"\"\n",
        "    pp(data, width=width)# List of model identifiers to query\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cce1aLBvctaL"
      },
      "source": [
        "### Setting Up API Keys\n",
        "\n",
        "Here we will securely set our API keys as environment variables. This is helpful because we don’t want to hardcode sensitive information (like API keys) directly into our code. By using environment variables, we can keep our credentials secure while still allowing our program to access them. Normally we would use a .env file to store our passwords to our enviroments, but since we are going to be working in colab we will do things a little different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BsK7GrHyV-c4",
        "outputId": "35fef9dc-e226-4e9d-e6c7-a597882b74f9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "os.environ['GROQ_API_KEY'] = getpass('Enter your GROQ API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m2mhu-VbSWfF"
      },
      "source": [
        "### Creating a Simple Chat Interaction with an AI Language Model\n",
        "This code initiates a chat interaction with a language model (specifically Groq’s LLaMA 3.2), where the model responds to the user's input. We use the aisuite library to communicate with the model and retrieve the response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBEOEq99eGjR",
        "outputId": "446fdba3-9072-4470-b3b8-627717013604"
      },
      "outputs": [],
      "source": [
        "import aisuiteplus as ai\n",
        "\n",
        "# Initialize the AI client for accessing the language model\n",
        "client = ai.Client()\n",
        "\n",
        "# Define a conversation with a system message and a user message\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful agent, who answers with brevity.\"},\n",
        "    {\"role\": \"user\", \"content\": 'Hi'},\n",
        "]\n",
        "\n",
        "# Request a response from the model\n",
        "response = client.chat.completions.create(model=\"groq:llama-3.2-3b-preview\", messages=messages)\n",
        "\n",
        "# Print the model's response\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YJSahowjiJBE"
      },
      "source": [
        "### Defining a Function to Interact with the Language Model\n",
        "\n",
        "This function, ask, streamlines the process of sending a user message to a language model and retrieving a response. It encapsulates the logic required to set up the conversation and can be reused throughout the notebook for different queries. It will not perserve any history or any continuing conversation.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n8DK8_RqqXFH"
      },
      "outputs": [],
      "source": [
        "def ask(message, sys_message=\"You are a helpful agent.\",\n",
        "         model=\"groq:llama-3.2-3b-preview\"):\n",
        "    # Initialize the AI client for accessing the language model\n",
        "    client = ai.Client()\n",
        "\n",
        "    # Construct the messages list for the chat\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": sys_message},\n",
        "        {\"role\": \"user\", \"content\": message}\n",
        "    ]\n",
        "\n",
        "    # Send the messages to the model and get the response\n",
        "    response = client.chat.completions.create(model=model, messages=messages)\n",
        "\n",
        "    # Return the content of the model's response\n",
        "    return response.choices[0].message.content\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "FGcqY4lBjtFj",
        "outputId": "0520933a-8f2f-4185-a8a2-c591283482a3"
      },
      "outputs": [],
      "source": [
        "ask(\"Hi. what is capital of Japan?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpeW6Pj6j_6H"
      },
      "source": [
        "The real value of AI Suite is the ablity to run a variety of different models.  Let's first set up a collection of different API keys which we can try out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9_kJlkGfj_NG",
        "outputId": "d45074c6-bbc6-4214-df0c-6d162a176f21"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = getpass('Enter your OPENAI API key: ')\n",
        "os.environ['ANTHROPIC_API_KEY'] = getpass('Enter your ANTHROPIC API key: ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mfPtlJlbTY6X"
      },
      "source": [
        "###Confirm each model is using a different provider\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHVESCGJuWWg",
        "outputId": "3102b43a-e754-4288-ec1d-9777791f25b6"
      },
      "outputs": [],
      "source": [
        "print(ask(\"Who is your creator?\"))\n",
        "print(ask('Who is your creator?', model='anthropic:claude-3-5-sonnet-20240620'))\n",
        "print(ask('Who is your creator?', model='openai:gpt-4o'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWBL4D2H2B_9"
      },
      "source": [
        "### Querying Multiple AI Models for a Common Question\n",
        "In this section, we will query several different versions of the LLaMA language model to get varied responses to the same question. This approach allows us to compare how different models handle the same prompt, providing insights into their performance and style."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_gg-sgYuoOb",
        "outputId": "d1c582ba-3471-4b0e-b9ca-317df8a1c1c5"
      },
      "outputs": [],
      "source": [
        "\n",
        "models = [\n",
        "    'llama-3.1-8b-instant',\n",
        "    'llama-3.2-1b-preview',\n",
        "    'llama-3.2-3b-preview',\n",
        "    'llama3-70b-8192',\n",
        "    'llama3-8b-8192'\n",
        "]\n",
        "\n",
        "# Initialize a list to hold the responses from each model\n",
        "ret = []\n",
        "\n",
        "# Loop through each model and get a response for the specified question\n",
        "for x in models:\n",
        "    ret.append(ask('Write a short one sentence explanation of the origins of AI?', model=f'groq:{x}'))\n",
        "\n",
        "# Print the model's name and its corresponding response\n",
        "for idx, x in enumerate(ret):\n",
        "    pprint(models[idx] + ': \\n ' + x + ' ')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8pnJPdD2NL0"
      },
      "source": [
        "### Querying Different AI Providers for a Common Question\n",
        "In this section, we will query multiple AI models from different providers to get varied responses to the same question regarding the origins of AI. This comparison allows us to observe how different models from different architectures respond to the same prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "j4TqhC5J1YIG",
        "outputId": "4a50e300-0a7a-4562-8a34-f31c4b9072d4"
      },
      "outputs": [],
      "source": [
        "# List of AI model providers to query\n",
        "providers = [\n",
        "    'groq:llama3-70b-8192',\n",
        "    'openai:gpt-4o',\n",
        "    'anthropic:claude-3-5-sonnet-20240620'\n",
        "]\n",
        "\n",
        "# Initialize a list to hold the responses from each provider\n",
        "ret = []\n",
        "\n",
        "# Loop through each provider and get a response for the specified question\n",
        "for x in providers:\n",
        "    ret.append(ask('Write a short one sentence explanation of the origins of AI?', model=x))\n",
        "\n",
        "# Print the provider's name and its corresponding response\n",
        "for idx, x in enumerate(ret):\n",
        "    pprint(providers[idx] + ': \\n' + x + ' \\n\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgPCC0y_U4WG"
      },
      "source": [
        "### Generating and Evaluating Questions with AI Models\n",
        "In this section, we will randomly generate questions using a language model and then have two other models provide answers to those questions. The user will then evaluate which answer is better, allowing for a comparative analysis of responses from different models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "fMx-TfLk09ft",
        "outputId": "56153c03-a1e6-4b72-fd16-b36197ccb5ee"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Initialize a list to store the best responses\n",
        "best = []\n",
        "\n",
        "# Loop to generate and evaluate questions\n",
        "for _ in range(20):\n",
        "    # Shuffle the providers list to randomly select models for each iteration\n",
        "    random.shuffle(providers)\n",
        "\n",
        "    # Generate a question using the first provider\n",
        "    question = ask('Please generate a short question that is suitable for asking an LLM.', model=providers[0])\n",
        "\n",
        "    # Get answers from the second and third providers\n",
        "    answer_1 = ask('Please give a short answer to this question: ' + question, model=providers[1])\n",
        "    answer_2 = ask('Please give a short answer to this question: ' + question, model=providers[2])\n",
        "\n",
        "    # Print the generated question and the two answers\n",
        "    pprint(f\"Original text:\\n  {question}\\n\\n\")\n",
        "    pprint(f\"Option 1 text:\\n  {answer_1}\\n\\n\")\n",
        "    pprint(f\"Option 2 text:\\n  {answer_2}\\n\\n\")\n",
        "\n",
        "    # Store the provider names and the user's choice of the best answer\n",
        "    best.append(str(providers) + ', ' + input(\"Which is best 1 or 2. 3 if indistinguishable: \"))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
